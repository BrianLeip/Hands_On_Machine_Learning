# EXERCISES 04 - 

1. What Linear Regression training algorithm can you use if you have a training set with millions of features?


2. Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?


3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?

4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?


Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?
Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?
Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?


Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn and TensorFlow (p. 146). O'Reilly Media. Kindle Edition. 